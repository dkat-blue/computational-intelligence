{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1]\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [2] Load and preprocess data\n",
    "def load_data(filepath, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for GMDH algorithm.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the data file\n",
    "        train_ratio: Ratio for training set (default 0.7)\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert Time to datetime with explicit format (day first)\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format='%d-%m-%Y %H:%M')\n",
    "    \n",
    "    # Select features (excluding Location)\n",
    "    feature_columns = ['Temp_2m', 'RelHum_2m', 'DP_2m', 'WS_10m', 'WS_100m', \n",
    "                      'WD_10m', 'WD_100m', 'WG_10m', 'Power']\n",
    "    \n",
    "    # Create feature matrix\n",
    "    data = df[feature_columns].values\n",
    "    \n",
    "    # Determine split indices\n",
    "    total_samples = data.shape[0]\n",
    "    train_end = int(train_ratio * total_samples)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:]\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on training data (excluding the target column)\n",
    "    scaled_train_features = scaler.fit_transform(train_data[:, :-1])\n",
    "    scaled_val_features = scaler.transform(val_data[:, :-1])\n",
    "    \n",
    "    # Combine scaled features with target\n",
    "    scaled_train_data = np.column_stack((scaled_train_features, train_data[:, -1]))\n",
    "    scaled_val_data = np.column_stack((scaled_val_features, val_data[:, -1]))\n",
    "    \n",
    "    print(\"Data shapes after scaling and splitting:\")\n",
    "    print(f\"Training data shape: {scaled_train_data.shape}\")\n",
    "    print(f\"Validation data shape: {scaled_val_data.shape}\")\n",
    "    \n",
    "    return scaled_train_data, scaled_val_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [3] Create sliding windows for GMDH\n",
    "def create_windows(data, window_size=12, target_col=-1):\n",
    "    \"\"\"\n",
    "    Create sliding windows from time series data for GMDH.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array of shape (n_samples, n_features)\n",
    "        window_size: number of time steps to use for input\n",
    "        target_col: index of target column (default: -1 for Power)\n",
    "    \n",
    "    Returns:\n",
    "        X: numpy array of shape (n_samples - window_size, window_size, n_features)\n",
    "        y: numpy array of shape (n_samples - window_size,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size):\n",
    "        window = data[i:(i + window_size)]\n",
    "        target = data[i + window_size, target_col]\n",
    "        \n",
    "        X.append(window)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [4] Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    train_data, val_data, scaler = load_data('../data/raw/Train.csv')\n",
    "    \n",
    "    # Create windows\n",
    "    X_train, y_train = create_windows(train_data)\n",
    "    X_val, y_val = create_windows(val_data)\n",
    "    \n",
    "    print(\"\\nWindowed dataset shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"X_val:   {X_val.shape}\")\n",
    "    print(f\"y_val:   {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [5] Updated PolynomialNeuron and fitting function with flattened inputs\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def fit_polynomial_neuron(X_train_in, y_train, feature_pair, polynomial_type):\n",
    "    \"\"\"\n",
    "    Fit a polynomial neuron given two input variables (from flattened windowed input).\n",
    "    X_train_in: shape (n_samples, W*F) where W is window_size and F is n_features.\n",
    "    \"\"\"\n",
    "    i, j = feature_pair\n",
    "    xi = X_train_in[:, i]\n",
    "    xj = X_train_in[:, j]\n",
    "\n",
    "    if polynomial_type == 'linear':\n",
    "        # f(x_i, x_j) = A0 + A1*x_i + A2*x_j\n",
    "        X_mat = np.column_stack([\n",
    "            np.ones_like(xi),\n",
    "            xi,\n",
    "            xj\n",
    "        ])\n",
    "    elif polynomial_type == 'incomplete_quadratic':\n",
    "        # f(x_i, x_j) = A0 + A1*x_i + A2*x_j + A3*(x_i*x_j)\n",
    "        X_mat = np.column_stack([\n",
    "            np.ones_like(xi),\n",
    "            xi,\n",
    "            xj,\n",
    "            xi * xj\n",
    "        ])\n",
    "    elif polynomial_type == 'full_quadratic':\n",
    "        # f(x_i, x_j) = A0 + A1*x_i + A2*x_j + A3*x_i*x_j + A4*x_i^2 + A5*x_j^2\n",
    "        X_mat = np.column_stack([\n",
    "            np.ones_like(xi),\n",
    "            xi,\n",
    "            xj,\n",
    "            xi * xj,\n",
    "            xi**2,\n",
    "            xj**2\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown polynomial_type\")\n",
    "\n",
    "    coeffs, _, _, _ = np.linalg.lstsq(X_mat, y_train, rcond=None)\n",
    "    return coeffs\n",
    "\n",
    "class PolynomialNeuron:\n",
    "    \"\"\"\n",
    "    Polynomial neuron now directly uses flattened inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_pair, coeffs, polynomial_type):\n",
    "        self.feature_pair = feature_pair\n",
    "        self.coeffs = coeffs\n",
    "        self.polynomial_type = polynomial_type\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X is (n_samples, W*F)\n",
    "        i, j = self.feature_pair\n",
    "        xi = X[:, i]\n",
    "        xj = X[:, j]\n",
    "\n",
    "        if self.polynomial_type == 'linear':\n",
    "            A0, A1, A2 = self.coeffs\n",
    "            return A0 + A1*xi + A2*xj\n",
    "        elif self.polynomial_type == 'incomplete_quadratic':\n",
    "            A0, A1, A2, A3 = self.coeffs\n",
    "            return A0 + A1*xi + A2*xj + A3*(xi*xj)\n",
    "        elif self.polynomial_type == 'full_quadratic':\n",
    "            A0, A1, A2, A3, A4, A5 = self.coeffs\n",
    "            return A0 + A1*xi + A2*xj + A3*(xi*xj) + A4*(xi**2) + A5*(xj**2)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown polynomial_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [6] Updated GMDHLayer\n",
    "\n",
    "class GMDHCriteria:\n",
    "    @staticmethod\n",
    "    def regularity_criterion(y_true, y_pred, nb):\n",
    "        return tf.reduce_sum(tf.square(y_true - y_pred)) / nb\n",
    "\n",
    "    @staticmethod\n",
    "    def unbiasedness_criterion(predictions1, predictions2, r1, r2):\n",
    "        # Simple approach as before\n",
    "        min_len = tf.minimum(tf.shape(predictions1)[0], tf.shape(predictions2)[0])\n",
    "        predictions1 = predictions1[:min_len]\n",
    "        predictions2 = predictions2[:min_len]\n",
    "        return tf.reduce_sum(tf.square(predictions1 - predictions2)) / tf.cast(min_len, tf.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def combined_criterion(regularity, unbiasedness, alpha):\n",
    "        return alpha * regularity + (1 - alpha) * unbiasedness\n",
    "\n",
    "\n",
    "class GMDHLayer:\n",
    "    def __init__(self, num_features, F=5, epsilon=1e-6, alpha=0.5, polynomial_type='full_quadratic'):\n",
    "        self.num_features = num_features\n",
    "        self.F = F\n",
    "        self.epsilon = epsilon\n",
    "        self.previous_criterion = None\n",
    "        self.neurons = []\n",
    "        self.feature_pairs = [(i, j) for i in range(num_features) for j in range(i+1, num_features)]\n",
    "        self.alpha = alpha\n",
    "        self.polynomial_type = polynomial_type\n",
    "\n",
    "    def fit_layer(self, X_train, y_train):\n",
    "        # Now X_train is already flattened: (n_samples, W*F)\n",
    "        self.neurons = []\n",
    "        for pair in self.feature_pairs:\n",
    "            coeffs = fit_polynomial_neuron(X_train, y_train, pair, self.polynomial_type)\n",
    "            neuron = PolynomialNeuron(feature_pair=pair, coeffs=coeffs, polynomial_type=self.polynomial_type)\n",
    "            self.neurons.append(neuron)\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        # X is flattened\n",
    "        preds = [neuron.predict(X) for neuron in self.neurons]\n",
    "        return np.column_stack(preds)\n",
    "\n",
    "    def check_stopping_criterion(self, current_criterion):\n",
    "        if self.previous_criterion is None:\n",
    "            self.previous_criterion = current_criterion\n",
    "            return False\n",
    "        diff = abs(current_criterion - self.previous_criterion)\n",
    "        self.previous_criterion = current_criterion\n",
    "        return diff < self.epsilon\n",
    "\n",
    "    def select_best_neurons(self, outputs_train, outputs_val, y_train, y_val):\n",
    "        y_val_t = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "        y_train_t = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "        out_train_t = tf.convert_to_tensor(outputs_train, dtype=tf.float32)\n",
    "        out_val_t = tf.convert_to_tensor(outputs_val, dtype=tf.float32)\n",
    "        nb_val = len(y_val)\n",
    "\n",
    "        criteria = []\n",
    "        for i in range(outputs_val.shape[1]):\n",
    "            reg = GMDHCriteria.regularity_criterion(y_val_t, out_val_t[:, i], nb_val)\n",
    "            unbias = GMDHCriteria.unbiasedness_criterion(out_train_t[:, i], out_val_t[:, i],\n",
    "                                                         len(y_train), len(y_val))\n",
    "            combined = GMDHCriteria.combined_criterion(reg, unbias, self.alpha)\n",
    "            criteria.append(combined)\n",
    "\n",
    "        criteria_tensor = tf.stack(criteria)\n",
    "        best_indices = tf.argsort(criteria_tensor)[:self.F]\n",
    "        return best_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [7] Updated GMDHModel\n",
    "\n",
    "class GMDHModel:\n",
    "    def __init__(self, input_dim, max_layers=5, F=5, epsilon=1e-6, alpha=0.5, polynomial_type='full_quadratic'):\n",
    "        self.max_layers = max_layers\n",
    "        self.F = F\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim  # Now this is W*F after flattening\n",
    "        self.polynomial_type = polynomial_type\n",
    "        self.godel_numbers = {}\n",
    "        self.gmdh_layers = []\n",
    "        self.best_neurons_per_layer = []\n",
    "        # For reconstructing final expression\n",
    "        self.feature_expressions = {i: f\"X_{i}\" for i in range(input_dim)}\n",
    "\n",
    "        self.history = {\n",
    "            'layer_criteria': [],\n",
    "            'regularity_criteria': [],\n",
    "            'unbiasedness_criteria': [],\n",
    "            'layer_outputs_train': [],\n",
    "            'layer_outputs_val': [],\n",
    "            'selected_neurons': [],\n",
    "            'layer_losses_train': [],\n",
    "            'layer_losses_val': []\n",
    "        }\n",
    "\n",
    "    def train_layer(self, X_train, y_train, X_val, y_val, layer_idx):\n",
    "        layer = GMDHLayer(num_features=X_train.shape[-1], F=self.F, epsilon=self.epsilon, alpha=self.alpha, polynomial_type=self.polynomial_type)\n",
    "        self.gmdh_layers.append(layer)\n",
    "\n",
    "        # Fit neurons\n",
    "        layer.fit_layer(X_train, y_train)\n",
    "\n",
    "        # Compute outputs\n",
    "        outputs_train = layer.predict_all(X_train)\n",
    "        outputs_val = layer.predict_all(X_val)\n",
    "\n",
    "        nb_val = len(y_val)\n",
    "        y_val_t = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "        y_train_t = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "        out_train_t = tf.convert_to_tensor(outputs_train, dtype=tf.float32)\n",
    "        out_val_t = tf.convert_to_tensor(outputs_val, dtype=tf.float32)\n",
    "\n",
    "        reg_criteria = []\n",
    "        unbias_criteria = []\n",
    "        combined_criteria = []\n",
    "\n",
    "        for i in range(outputs_train.shape[1]):\n",
    "            reg = GMDHCriteria.regularity_criterion(y_val_t, out_val_t[:, i], nb_val)\n",
    "            unb = GMDHCriteria.unbiasedness_criterion(out_train_t[:, i], out_val_t[:, i],\n",
    "                                                      len(y_train), len(y_val))\n",
    "            comb = GMDHCriteria.combined_criterion(reg, unb, self.alpha)\n",
    "            reg_criteria.append(reg.numpy())\n",
    "            unbias_criteria.append(unb.numpy())\n",
    "            combined_criteria.append(comb.numpy())\n",
    "\n",
    "        # Select best neurons\n",
    "        best_indices = layer.select_best_neurons(outputs_train, outputs_val, y_train, y_val)\n",
    "        self.best_neurons_per_layer.append(best_indices)\n",
    "\n",
    "        self.history['layer_criteria'].append(combined_criteria)\n",
    "        self.history['regularity_criteria'].append(reg_criteria)\n",
    "        self.history['unbiasedness_criteria'].append(unbias_criteria)\n",
    "        self.history['layer_outputs_train'].append(outputs_train)\n",
    "        self.history['layer_outputs_val'].append(outputs_val)\n",
    "        self.history['selected_neurons'].append(best_indices)\n",
    "\n",
    "        y_train_2d = y_train[:, np.newaxis]\n",
    "        y_val_2d = y_val[:, np.newaxis]\n",
    "        train_loss = np.mean((y_train_2d - outputs_train)**2)\n",
    "        val_loss = np.mean((y_val_2d - outputs_val)**2)\n",
    "        self.history['layer_losses_train'].append(train_loss)\n",
    "        self.history['layer_losses_val'].append(val_loss)\n",
    "\n",
    "        # Update expressions\n",
    "        if layer_idx == 0:\n",
    "            prev_features = self.feature_expressions\n",
    "        else:\n",
    "            prev_features = self.feature_expressions\n",
    "\n",
    "        new_feature_expressions = {}\n",
    "        for i, neuron_idx in enumerate(best_indices):\n",
    "            neuron = layer.neurons[neuron_idx]\n",
    "            fp = neuron.feature_pair\n",
    "            f1_expr = self.feature_expressions[fp[0]]\n",
    "            f2_expr = self.feature_expressions[fp[1]]\n",
    "\n",
    "            if self.polynomial_type == 'linear':\n",
    "                A0, A1, A2 = neuron.coeffs\n",
    "                expr = f\"({A0:.4f} + {A1:.4f}*{f1_expr} + {A2:.4f}*{f2_expr})\"\n",
    "            elif self.polynomial_type == 'incomplete_quadratic':\n",
    "                A0, A1, A2, A3 = neuron.coeffs\n",
    "                expr = f\"({A0:.4f} + {A1:.4f}*{f1_expr} + {A2:.4f}*{f2_expr} + {A3:.4f}*({f1_expr}*{f2_expr}))\"\n",
    "            elif self.polynomial_type == 'full_quadratic':\n",
    "                A0, A1, A2, A3, A4, A5 = neuron.coeffs\n",
    "                expr = f\"({A0:.4f} + {A1:.4f}*{f1_expr} + {A2:.4f}*{f2_expr} + {A3:.4f}*({f1_expr}*{f2_expr}) + {A4:.4f}*({f1_expr}^2) + {A5:.4f}*({f2_expr}^2))\"\n",
    "            else:\n",
    "                raise ValueError(\"Unknown polynomial_type\")\n",
    "\n",
    "            new_feature_expressions[i] = expr\n",
    "\n",
    "        self.feature_expressions = new_feature_expressions\n",
    "        current_criterion = np.mean([combined_criteria[i] for i in best_indices])\n",
    "        return layer.check_stopping_criterion(current_criterion)\n",
    "\n",
    "    def synthesize(self, X_train, y_train, X_val, y_val):\n",
    "        current_X_train = X_train\n",
    "        current_X_val = X_val\n",
    "\n",
    "        for layer_idx in range(self.max_layers):\n",
    "            print(f\"Synthesizing layer {layer_idx + 1}\")\n",
    "            should_stop = self.train_layer(current_X_train, y_train, current_X_val, y_val, layer_idx)\n",
    "            if should_stop:\n",
    "                print(f\"Stopping criterion met at layer {layer_idx + 1}\")\n",
    "                break\n",
    "\n",
    "            last_layer = self.gmdh_layers[-1]\n",
    "            outputs_train = last_layer.predict_all(current_X_train)\n",
    "            outputs_val = last_layer.predict_all(current_X_val)\n",
    "            best_indices = self.best_neurons_per_layer[-1]\n",
    "\n",
    "            # Update the input for the next layer (now just one dimension)\n",
    "            current_X_train = outputs_train[:, best_indices]\n",
    "            current_X_val = outputs_val[:, best_indices]\n",
    "\n",
    "    def predict(self, X):\n",
    "        x = X\n",
    "        for layer_idx, layer in enumerate(self.gmdh_layers):\n",
    "            out = layer.predict_all(x)\n",
    "            best_indices = self.best_neurons_per_layer[layer_idx]\n",
    "            out = out[:, best_indices]\n",
    "            x = out\n",
    "        return out\n",
    "\n",
    "    def get_final_model_expression(self):\n",
    "        return list(self.feature_expressions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [8] Updated main execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    train_data, val_data, scaler = load_data('../data/raw/Train.csv')\n",
    "    \n",
    "    # Create windows\n",
    "    X_train, y_train = create_windows(train_data)\n",
    "    X_val, y_val = create_windows(val_data)\n",
    "    \n",
    "    # Flatten the windowed data: \n",
    "    # If X_train is (N, W, F), flatten to (N, W*F)\n",
    "    N_train, W, F = X_train.shape\n",
    "    X_train_flat = X_train.reshape(N_train, W*F)\n",
    "    N_val = X_val.shape[0]\n",
    "    X_val_flat = X_val.reshape(N_val, W*F)\n",
    "\n",
    "    # Initialize and train model with given alpha and polynomial_type\n",
    "    model = GMDHModel(input_dim=X_train_flat.shape[-1], max_layers=5, F=5, epsilon=1e-6, alpha=0.5, polynomial_type='full_quadratic')\n",
    "    \n",
    "    # Train the model\n",
    "    model.synthesize(X_train_flat, y_train, X_val_flat, y_val)\n",
    "    \n",
    "    # Print final layer performance\n",
    "    print(\"\\nFinal Layer Performance:\")\n",
    "    print(\"Training Loss:\", model.history['layer_losses_train'][-1])\n",
    "    print(\"Validation Loss:\", model.history['layer_losses_val'][-1])\n",
    "    \n",
    "    # Example predictions\n",
    "    predictions = model.predict(X_val_flat)\n",
    "    predictions_mean = predictions.mean(axis=1)\n",
    "    print(\"\\nSample Predictions:\", predictions_mean[:5])\n",
    "\n",
    "    # Print final model expression\n",
    "    print(\"\\nFinal Model Expression(s):\")\n",
    "    final_expressions = model.get_final_model_expression()\n",
    "    for expr in final_expressions:\n",
    "        print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [9] Evaluation and Visualization for GMDH Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Since the model expects flattened inputs, ensure we use X_val_flat:\n",
    "# If X_val_flat is not defined here, we can flatten X_val:\n",
    "# X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Get predictions from the model\n",
    "predictions_val = model.predict(X_val_flat)  # shape: (n_samples, F)\n",
    "predictions_val_mean = predictions_val.mean(axis=1)  # Average across neurons\n",
    "\n",
    "# Compute validation metrics\n",
    "r2_val = r2_score(y_val, predictions_val_mean)\n",
    "mse_val = mean_squared_error(y_val, predictions_val_mean)\n",
    "mae_val = mean_absolute_error(y_val, predictions_val_mean)\n",
    "\n",
    "print(\"Validation R²:\", r2_val)\n",
    "print(\"Validation MSE:\", mse_val)\n",
    "print(\"Validation MAE:\", mae_val)\n",
    "\n",
    "# Scatter Plot: Predicted vs Actual (Validation Set)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_val, predictions_val_mean, alpha=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Power')\n",
    "plt.ylabel('Predicted Power')\n",
    "plt.title('GMDH Model: Predicted vs. Actual (Validation Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals_val = y_val - predictions_val_mean\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals_val, kde=True, bins=50)\n",
    "plt.xlabel('Residual (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals (Validation Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Line Plot: Actual vs Predicted Over Time (Validation Set)\n",
    "start_idx = 0\n",
    "end_idx = min(2000, len(y_val))  # adjust as needed\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(start_idx, end_idx), y_val[start_idx:end_idx], label='Actual Power', linewidth=2)\n",
    "plt.plot(range(start_idx, end_idx), predictions_val_mean[start_idx:end_idx], label='Predicted Power', linewidth=2)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Power')\n",
    "plt.title('GMDH Model: Actual vs. Predicted Power Over Time (Validation Set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
