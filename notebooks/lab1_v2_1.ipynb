{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "train_file_path = os.path.join('..', 'data', 'raw', 'Train.csv')\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "\n",
    "print(\"\\nFirst few rows of the train dataset:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nTrain dataset info:\")\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "train_data.drop(columns=['Unnamed: 0', 'Time', 'Location'], inplace=True, errors='ignore')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = ['Temp_2m', 'RelHum_2m', 'DP_2m', 'WS_10m',\n",
    "                 'WS_100m', 'WD_10m', 'WD_100m', 'WG_10m']\n",
    "output_column = 'Power'\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_val_data, test_data = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "val_size_adjusted = 0.15 / (1 - 0.15)  # Adjust validation size relative to the training set\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=val_size_adjusted, random_state=SEED)\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", val_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n",
    "\n",
    "# Separate input features and target variable\n",
    "train_X = train_data[input_columns]\n",
    "train_y = train_data[[output_column]]\n",
    "\n",
    "val_X = val_data[input_columns]\n",
    "val_y = val_data[[output_column]]\n",
    "\n",
    "test_X = test_data[input_columns]\n",
    "test_y = test_data[[output_column]]\n",
    "\n",
    "# Scale the input features\n",
    "scaler_X = StandardScaler()\n",
    "train_X_scaled = scaler_X.fit_transform(train_X)\n",
    "val_X_scaled = scaler_X.transform(val_X)\n",
    "test_X_scaled = scaler_X.transform(test_X)\n",
    "\n",
    "# Scale the target variable\n",
    "scaler_y = StandardScaler()\n",
    "train_y_scaled = scaler_y.fit_transform(train_y)\n",
    "val_y_scaled = scaler_y.transform(val_y)\n",
    "test_y_scaled = scaler_y.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model with increased capacity\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=(train_X_scaled.shape[1],)),\n",
    "    Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Linear activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model with SGD optimizer with momentum and a learning rate scheduler\n",
    "optimizer = SGD(learning_rate=1e-3, momentum=0.9)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Implement ReduceLROnPlateau learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Add early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                               restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model for more epochs with progress tracking\n",
    "history = model.fit(\n",
    "    train_X_scaled, train_y_scaled,\n",
    "    validation_data=(val_X_scaled, val_y_scaled),\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(test_X_scaled, test_y_scaled, verbose=0)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions_scaled = model.predict(test_X_scaled)\n",
    "test_predictions = scaler_y.inverse_transform(test_predictions_scaled)\n",
    "test_actual = scaler_y.inverse_transform(test_y_scaled)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test_actual, test_predictions))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened weight arrays helper functions\n",
    "def get_weights_flat(model):\n",
    "    # Get model weights and flatten them into a 1D array\n",
    "    weights = model.get_weights()\n",
    "    flat_weights = np.concatenate([w.flatten() for w in weights])\n",
    "    return flat_weights\n",
    "\n",
    "def set_weights_from_flat(model, flat_weights):\n",
    "    # Set model weights from a flat 1D array\n",
    "    weights = []\n",
    "    idx = 0\n",
    "    for layer in model.layers:\n",
    "        for w in layer.get_weights():\n",
    "            shape = w.shape\n",
    "            size = np.prod(shape)\n",
    "            new_w = flat_weights[idx:idx+size].reshape(shape)\n",
    "            weights.append(new_w)\n",
    "            idx += size\n",
    "    model.set_weights(weights)\n",
    "\n",
    "# Simplified neural network architecture\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(train_X_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)  # Linear activation for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the fitness function\n",
    "def compute_fitness(E_Wi):\n",
    "    # FI(W_i) = 1 / (E_Wi + epsilon)\n",
    "    epsilon = 1e-8  # Small constant to prevent division by zero\n",
    "    FI_Wi = 1 / (E_Wi + epsilon)\n",
    "    return FI_Wi\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "population_size = 50  # Increased population size\n",
    "max_generations = 200\n",
    "mutation_rate = 0.005  # Reduced mutation rate\n",
    "tolerance = 1e-4  # Tolerance for early stopping\n",
    "tournament_size = 3  # For tournament selection\n",
    "\n",
    "# Initialize the base model and population\n",
    "base_model = create_model()\n",
    "base_model.build((None, train_X_scaled.shape[1]))\n",
    "\n",
    "# Get the total number of weights in the model\n",
    "initial_weights_flat = get_weights_flat(base_model)\n",
    "num_weights = len(initial_weights_flat)\n",
    "\n",
    "# Initialize population with random weights\n",
    "population = []\n",
    "for _ in range(population_size):\n",
    "    # Randomly initialize weights\n",
    "    individual = np.random.randn(num_weights) * 0.1\n",
    "    population.append(individual)\n",
    "\n",
    "# Genetic Algorithm loop\n",
    "best_fitness_overall = -np.inf\n",
    "best_individual_overall = None\n",
    "fitness_history = []\n",
    "\n",
    "for generation in range(max_generations):\n",
    "    print(f\"Generation {generation}\")\n",
    "    # Evaluate fitness for each individual\n",
    "    fitnesses = []\n",
    "    for individual in population:\n",
    "        # Set model weights\n",
    "        set_weights_from_flat(base_model, individual)\n",
    "        # Compute loss on training data\n",
    "        y_pred = base_model.predict(train_X_scaled, verbose=0)\n",
    "        E_Wi = mean_squared_error(train_y_scaled, y_pred)\n",
    "        FI_Wi = compute_fitness(E_Wi)\n",
    "        fitnesses.append(FI_Wi)\n",
    "    fitnesses = np.array(fitnesses)\n",
    "\n",
    "    # Keep track of the best individual\n",
    "    best_fitness_idx = np.argmax(fitnesses)\n",
    "    best_fitness = fitnesses[best_fitness_idx]\n",
    "    best_individual = population[best_fitness_idx]\n",
    "    print(f\"Best fitness: {best_fitness:.6f}\")\n",
    "    fitness_history.append(best_fitness)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if best_fitness > best_fitness_overall:\n",
    "        best_fitness_overall = best_fitness\n",
    "        best_individual_overall = best_individual.copy()\n",
    "    if 1 / best_fitness_overall < tolerance:\n",
    "        print(\"Early stopping criteria met\")\n",
    "        break\n",
    "\n",
    "    # Tournament selection\n",
    "    def tournament_selection(population, fitnesses, k):\n",
    "        selected = []\n",
    "        for _ in range(len(population)):\n",
    "            participants = np.random.choice(len(population), k, replace=False)\n",
    "            best_idx = participants[np.argmax(fitnesses[participants])]\n",
    "            selected.append(population[best_idx])\n",
    "        return selected\n",
    "\n",
    "    selected_population = tournament_selection(population, fitnesses, tournament_size)\n",
    "\n",
    "    # Generate new population\n",
    "    new_population = []\n",
    "    while len(new_population) < population_size:\n",
    "        # Select parents\n",
    "        parent1, parent2 = random.sample(selected_population, 2)\n",
    "        # Uniform crossover\n",
    "        mask = np.random.rand(num_weights) < 0.5\n",
    "        offspring1 = np.where(mask, parent1, parent2)\n",
    "        offspring2 = np.where(mask, parent2, parent1)\n",
    "        # Mutation\n",
    "        mutation_mask1 = np.random.rand(num_weights) < mutation_rate\n",
    "        mutation_values1 = np.random.randn(num_weights) * 0.1\n",
    "        offspring1[mutation_mask1] += mutation_values1[mutation_mask1]\n",
    "\n",
    "        mutation_mask2 = np.random.rand(num_weights) < mutation_rate\n",
    "        mutation_values2 = np.random.randn(num_weights) * 0.1\n",
    "        offspring2[mutation_mask2] += mutation_values2[mutation_mask2]\n",
    "\n",
    "        new_population.extend([offspring1, offspring2])\n",
    "\n",
    "    # Ensure population size does not exceed the limit\n",
    "    population = new_population[:population_size]\n",
    "\n",
    "# After GA, set the best individual's weights and evaluate on the test set\n",
    "set_weights_from_flat(base_model, best_individual_overall)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = base_model.predict(test_X_scaled)\n",
    "test_loss = mean_squared_error(test_y_scaled, y_pred_test)\n",
    "print(f\"Test Loss (MSE): {test_loss:.6f}\")\n",
    "\n",
    "# Compute RMSE\n",
    "test_actual = scaler_y.inverse_transform(test_y_scaled)\n",
    "test_predictions = scaler_y.inverse_transform(y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(test_actual, test_predictions))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Plot fitness over generations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fitness_history)\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Best Fitness')\n",
    "plt.title('Best Fitness Over Generations')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
